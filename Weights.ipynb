{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c1751e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import statistics\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from scipy.stats import kurtosis, skew, linregress\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b453c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df = df.loc[:, df.columns.intersection([\"block_time\", \"address\", \"volume\", \"fee\", \"pnl\", \"return\", \"liquidation\"])]\n",
    "# Only the needed bit of time value\n",
    "df[\"block_time\"] = df[\"block_time\"].apply(lambda x: x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49981374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize a list of values to the range [0, 1]\n",
    "def normalize_values(values):\n",
    "    # Find the minimum and maximum values in the list\n",
    "    min_value, max_value = min(values), max(values)\n",
    "    \n",
    "    # Normalize each value using the min-max scaling formula\n",
    "    normalized_values = [(value - min_value) / (max_value - min_value) for value in values]\n",
    "    \n",
    "    return normalized_values\n",
    "\n",
    "# Function to generate random weights for a given number of items, ensuring they sum to 1\n",
    "def generate_weights(n, min_value=0.0, max_value=1.0):\n",
    "    # Check if the provided range of weights is feasible\n",
    "    if min_value * n > 1 or max_value * n < 1:\n",
    "        return \"Weights are not feasible.\"\n",
    "\n",
    "    remaining_weights = 1  # Initialize the remaining weight to be distributed\n",
    "    weights = []  # Initialize an empty list to store the generated weights\n",
    "\n",
    "    # Generate 'n-1' random weights to ensure their sum is less than or equal to 1\n",
    "    for _ in range(n - 1):\n",
    "        weight = random.uniform(min_value, min(remaining_weights, max_value))\n",
    "        weights.append(weight)  # Append the generated weight to the list\n",
    "        remaining_weights -= weight  # Update the remaining weight\n",
    "\n",
    "    # Add the last weight to ensure that all weights sum to exactly 1\n",
    "    weights.append(remaining_weights)\n",
    "\n",
    "    # Shuffle the weights to randomize their order\n",
    "    random.shuffle(weights)\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c9a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for regression tests\n",
    "class RegressionTest:\n",
    "    \n",
    "    # Constructor to initialize the class with data\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    # Function to calculate the regression coefficient for a given address\n",
    "    def calculate_regression_coefficient(self, address):\n",
    "        # Filter the data for the specified address\n",
    "        df_filtered = self.data[self.data[\"address\"] == address]\n",
    "        \n",
    "        # Create x and y arrays for regression\n",
    "        x = np.arange(1, len(df_filtered) + 1)\n",
    "        y = np.cumsum(df_filtered['return'])\n",
    "        \n",
    "        # Calculate linear regression statistics\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        \n",
    "        # Normalize the slope\n",
    "        normalized_slope = (np.rad2deg(np.arctan(slope)) + 90) / 180\n",
    "        \n",
    "        # Calculate the regression coefficient\n",
    "        coefficient = (r_value ** 2 * normalized_slope) / (std_err + 1)\n",
    "        \n",
    "        # Adjust the coefficient if needed\n",
    "        coefficient /= (1 - p_value) if 1 - p_value != 0 else 1\n",
    "        \n",
    "        return coefficient\n",
    "\n",
    "    # Function to calculate and normalize regression coefficients for all unique addresses\n",
    "    def Normalized_Regression_coefficient(self):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        regression_coefficient_values = [self.calculate_regression_coefficient(address) for address in addresses]\n",
    "        normalized_regression_coefficient_values = normalize_values(regression_coefficient_values)\n",
    "        return dict(zip(addresses, normalized_regression_coefficient_values))\n",
    "    \n",
    "    # Function to add the normalized regression coefficients to a CSV file\n",
    "    def addToCSV(self, file):\n",
    "        data = []\n",
    "        NRC = self.Normalized_Regression_coefficient()\n",
    "        \n",
    "        # Create a list of dictionaries for CSV writing\n",
    "        for i in range(len(NRC)):\n",
    "            dic = {}\n",
    "            dic[\"Address\"] = list(NRC.keys())[i]\n",
    "            dic[\"Normalized Regression Coefficient\"] = list(NRC.values())[i]\n",
    "            data.append(dic)\n",
    "        \n",
    "        # Write the data to the CSV file\n",
    "        with open(file, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "\n",
    "# Create an instance of the RegressionTest class with the provided DataFrame 'df'\n",
    "RT = RegressionTest(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0df701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - NLC:0.047993521956239016\n",
      "2 - NCVNR:0.4523533051742929\n",
      "3 - NCV:0.7907727419680378\n",
      "4 - NV:0.8839590500515858\n"
     ]
    }
   ],
   "source": [
    "# Define a class for risk tests\n",
    "class RiskTest:\n",
    "    \n",
    "    # Constructor to initialize the class with data\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    # Function to calculate the Coefficient of Variation (CV) for negative returns of a given address\n",
    "    def calculate_negative_CV_returns(self, address):\n",
    "        # Extract negative returns for the specified address\n",
    "        returns = self.data[self.data[\"address\"] == address][\"return\"].values.tolist()\n",
    "        negative_returns = [r for r in returns if r < 0]\n",
    "        \n",
    "        # Calculate mean and standard deviation of negative returns\n",
    "        mean_negative_returns = np.mean(negative_returns)\n",
    "        std_dev_negative_returns = np.std(negative_returns)\n",
    "        \n",
    "        # Calculate CV for negative returns\n",
    "        cv_negative_returns = std_dev_negative_returns / mean_negative_returns\n",
    "        return cv_negative_returns\n",
    "        \n",
    "    # Function to normalize CV values for negative returns of all unique addresses\n",
    "    def Normalized_CV_negative_returns(self):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        cv_negative_returns = [self.calculate_negative_CV_returns(address) for address in addresses]\n",
    "        normalized_cv_negative_returns = normalize_values(cv_negative_returns)\n",
    "        return dict(zip(addresses, normalized_cv_negative_returns))\n",
    "\n",
    "    # Function to calculate Value at Risk (VaR) for a given address and confidence level\n",
    "    def calculate_var(self, address, confidence_level):\n",
    "        df_filtered = self.data[self.data[\"address\"] == address]\n",
    "        returns = np.array(df_filtered[\"return\"])\n",
    "        sorted_returns = np.sort(returns)\n",
    "        index = int(np.floor(confidence_level * len(sorted_returns)))\n",
    "        var = sorted_returns[index]\n",
    "        return var\n",
    "\n",
    "    # Function to normalize VaR values for all unique addresses at a specified confidence level\n",
    "    def Normalized_VaR(self, confidence_level=0.05):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        var_values = [self.calculate_var(address, confidence_level) for address in addresses]\n",
    "        normalized_var_values = normalize_values(var_values)\n",
    "        return dict(zip(addresses, normalized_var_values))\n",
    "    \n",
    "    # Function to calculate Conditional Value at Risk (cVaR) for a given address and confidence level\n",
    "    def calculate_cvar(self, address, confidence_level):\n",
    "        df_filtered = self.data[(self.data[\"address\"] == address) & (self.data[\"pnl\"] < 0)]\n",
    "        returns = np.array(df_filtered[\"return\"])\n",
    "        cvar = np.nan if len(returns) == 0 else np.mean(returns)\n",
    "        return cvar\n",
    "\n",
    "    # Function to normalize cVaR values for all unique addresses at a specified confidence level\n",
    "    def Normalized_cVaR(self, confidence_level=0.05):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        cvar_values = [self.calculate_cvar(address, confidence_level) for address in addresses]\n",
    "        normalized_cvar_values = normalize_values(cvar_values)\n",
    "        return dict(zip(addresses, normalized_cvar_values))\n",
    "    \n",
    "    # Function to calculate the probability of liquidation for a given address\n",
    "    def calculate_liquidation_chance(self, address):\n",
    "        df_filtered = self.data[self.data[\"address\"] == address]\n",
    "        total_trades = len(df_filtered)\n",
    "        liquidation_counts = df_filtered[\"liquidation\"].apply(lambda x: 1 if x == \"Yes\" else 0).sum()\n",
    "        liquidation_chance = liquidation_counts / total_trades\n",
    "        return liquidation_chance\n",
    "\n",
    "    # Function to normalize liquidation chances for all unique addresses\n",
    "    def Normalized_Liquidation_chance(self):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        liquidation_chance_values = [self.calculate_liquidation_chance(address) for address in addresses]\n",
    "        normalized_liquidation_chance_values = normalize_values(liquidation_chance_values)    \n",
    "        return dict(zip(addresses, normalized_liquidation_chance_values))\n",
    "    \n",
    "    # Function to calculate the maximum drawdown for a given address\n",
    "    def calculate_max_drawdown(self, address):\n",
    "        filtered_data = self.data[self.data['address'] == address]\n",
    "        cumulative_returns = np.cumprod(1 + filtered_data['return']) - 1\n",
    "        peaks = np.maximum.accumulate(cumulative_returns)\n",
    "        troughs = np.minimum.accumulate(cumulative_returns)\n",
    "        drawdowns = peaks - troughs\n",
    "        max_drawdown = np.max(drawdowns)\n",
    "        return max_drawdown\n",
    "    \n",
    "    # Function to normalize maximum drawdown values for all unique addresses\n",
    "    def Normalized_Max_Drawdown(self):\n",
    "        values = normalize_values([self.calculate_max_drawdown(address) for address in self.data[\"address\"].unique()])\n",
    "        risk_df = {}\n",
    "        for i, address in enumerate(self.data[\"address\"].unique()):\n",
    "            risk_df[address] = values[i]\n",
    "        return risk_df\n",
    "    \n",
    "    # Function to add risk test results to a CSV file\n",
    "    def addToCSV(self, file):\n",
    "        data = []\n",
    "        NCVNR = self.Normalized_CV_negative_returns()\n",
    "        NV = self.Normalized_VaR()\n",
    "        NCV = self.Normalized_cVaR()\n",
    "        NLC = self.Normalized_Liquidation_chance()\n",
    "        current = 0\n",
    "        weights = [0.3, 0.3, 0.3, 0.1]\n",
    "        dic = {}\n",
    "        for i in range(len(NV)):\n",
    "            dic[\"Address\"] = list(NV.keys())[i]\n",
    "            dic[\"Normalized VaR\"] = list(NV.values())[i]\n",
    "            dic[\"Normalized CVaR\"] = list(NCV.values())[i]\n",
    "            dic[\"Normalized Liquidation Chance\"] = list(NLC.values())[i]\n",
    "            dic[\"Weighted\"] = (\n",
    "                list(NV.values())[i] * weights[1] +\n",
    "                list(NCV.values())[i] * weights[2] +\n",
    "                list(NLC.values())[i] * weights[0]\n",
    "            )\n",
    "        with open(file, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=dic[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(dic)\n",
    "\n",
    "    # Function to run multiple simulations with different weights and find the best combination\n",
    "    def runMultiple(self, simulations=100):\n",
    "        NCVNR = self.Normalized_CV_negative_returns()\n",
    "        NV = self.Normalized_VaR()\n",
    "        NCV = self.Normalized_cVaR()\n",
    "        NLC = self.Normalized_Liquidation_chance()\n",
    "        data = []\n",
    "        for j in range(simulations):\n",
    "            weights = generate_weights(4, 0.20, 0.45)\n",
    "            dic = {}\n",
    "            for key in NCV.keys():\n",
    "                dic[key] = (\n",
    "                    NCVNR[key] * weights[0] +\n",
    "                    NV[key] * weights[1] +\n",
    "                    NCV[key] * weights[2] +\n",
    "                    NLC[key] * weights[3]\n",
    "                )\n",
    "            data.append(dic)\n",
    "        drawdowns = self.Normalized_Max_Drawdown()\n",
    "        drawdowns = {key: drawdowns[key] for key in data[0]}\n",
    "        curr, currentMinimumMSE = None, float('inf')\n",
    "        for nvs in data:\n",
    "            totalSquaredError = sum([(nvs[key] - drawdowns[key]) ** 2 for key, value in nvs.items()])\n",
    "            meanSquaredError = totalSquaredError / len(nvs) \n",
    "            if meanSquaredError < currentMinimumMSE:\n",
    "                currentMinimumMSE, curr = meanSquaredError, nvs\n",
    "        print(weights)\n",
    "        \n",
    "    # Function to rank the weights based on their mean squared errors\n",
    "    def rankWeights(self):\n",
    "        NCVNR = self.Normalized_CV_negative_returns()\n",
    "        NV = self.Normalized_VaR()\n",
    "        NCV = self.Normalized_cVaR()\n",
    "        NLC = self.Normalized_Liquidation_chance()\n",
    "        drawdowns = self.Normalized_Max_Drawdown()\n",
    "        drawdowns = {key: drawdowns[key] for key in NV.keys()}\n",
    "        \n",
    "        dictionary = {}\n",
    "        dictionary[\"NCVNR\"] = sum([(NCVNR[key] - drawdowns[key]) ** 2 for key in NCVNR.keys()]) / len(NCVNR)\n",
    "        dictionary[\"NV\"] = sum([(NV[key] - drawdowns[key]) ** 2 for key in NV.keys()]) / len(NV)\n",
    "        dictionary[\"NCV\"] = sum([(NCV[key] - drawdowns[key]) ** 2 for key in NCV.keys()]) / len(NCV)\n",
    "        dictionary[\"NLC\"] = sum([(NLC[key] - drawdowns[key]) ** 2 for key in NLC.keys()]) / len(NLC)\n",
    "        \n",
    "        # Sort the weights based on mean squared errors\n",
    "        dictionary = dict(sorted(dictionary.items(), key=lambda item: item[1]))\n",
    "        \n",
    "        for i, item in enumerate(dictionary.items()):\n",
    "            print(f\"{i + 1} - {item[0]}:{item[1]}\")\n",
    "\n",
    "# Create an instance of the RiskTest class with the provided DataFrame 'df'\n",
    "RT = RiskTest(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63f76931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - NPV:0.021273964493066882\n",
      "2 - NPC:0.021273964493066892\n",
      "3 - NER:0.028792707927005372\n"
     ]
    }
   ],
   "source": [
    "# Define a class for profitability tests\n",
    "class ProfitabilityTest:\n",
    "    \n",
    "    # Constructor to initialize the class with data\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    # Function to calculate PNL to volume ratio for a given address\n",
    "    def calculate_pnltovolume(self, address):\n",
    "        df_filtered = self.data[self.data[\"address\"] == address]\n",
    "        if len(df_filtered) == 0:\n",
    "            return None\n",
    "        total_pnl = sum(df_filtered['pnl'])\n",
    "        total_volume = sum(df_filtered['volume'])    \n",
    "        return total_pnl / total_volume\n",
    "\n",
    "    # Function to normalize PNL to volume ratios for all unique addresses\n",
    "    def Normalized_Pnltovolume_y(self):\n",
    "        addresses = self.data['address'].unique()\n",
    "        pnltovolume_values = [self.calculate_pnltovolume(address) for address in addresses]\n",
    "        normalized_pnltovolume_values = normalize_values(pnltovolume_values)\n",
    "        return dict(zip(addresses, normalized_pnltovolume_values))\n",
    "    \n",
    "    # Function to calculate PNL to collateral ratio for a given address\n",
    "    def calculate_pnltocollateral(self, address):\n",
    "        grouped = self.data.groupby(\"address\").agg({\"pnl\": \"sum\", \"volume\": \"sum\"})\n",
    "        grouped[\"pnltocollateral\"] = grouped[\"pnl\"] / grouped[\"volume\"]\n",
    "        return grouped.loc[address, \"pnltocollateral\"]\n",
    "\n",
    "    # Function to normalize PNL to collateral ratios for all unique addresses\n",
    "    def Normalized_Pnltocollateral(self):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        pnltocollateral_values = [self.calculate_pnltocollateral(address) for address in addresses]\n",
    "        normalized_pnltocollateral_values = normalize_values(pnltocollateral_values)\n",
    "        return dict(zip(addresses, normalized_pnltocollateral_values))\n",
    "    \n",
    "    # Function to calculate expected return for a given address\n",
    "    def calculate_expected_return(self, address):\n",
    "        expected_return = self.data[self.data[\"address\"] == address][\"return\"].mean()\n",
    "        return expected_return\n",
    "\n",
    "    # Function to normalize expected return values for all unique addresses\n",
    "    def Normalized_Expected_return_y(self):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        expected_return_values = [self.calculate_expected_return(address) for address in addresses]\n",
    "        normalized_expected_return_values = normalize_values(expected_return_values)\n",
    "        return dict(zip(addresses, normalized_expected_return_values))\n",
    "    \n",
    "    # Function to calculate Sharpe ratio for a given address\n",
    "    def calculate_sharpe_ratio(self, address, risk_free_rate=0.02):\n",
    "        filtered_data = self.data[self.data['address'] == address]\n",
    "        excess_returns = filtered_data['return'] - risk_free_rate\n",
    "        sharpe_ratio = np.mean(excess_returns) / np.std(excess_returns)\n",
    "        return sharpe_ratio\n",
    "    \n",
    "    # Function to normalize Sharpe ratios for all unique addresses\n",
    "    def Normalized_Sharpe_Ratio(self):\n",
    "        values = normalize_values([self.calculate_sharpe_ratio(address) for address in self.data[\"address\"].unique()])\n",
    "        profitability_df = {}\n",
    "        for i, address in enumerate(self.data[\"address\"].unique()):\n",
    "            profitability_df[address] = values[i]\n",
    "        return profitability_df\n",
    "    \n",
    "    # Function to add profitability test results to a CSV file\n",
    "    def addToCSV(self, file):\n",
    "        data = []\n",
    "        NPV = self.Normalized_Pnltovolume_y()\n",
    "        NPC = self.Normalized_Pnltocollateral()\n",
    "        NER = self.Normalized_Expected_return_y()\n",
    "        current = 0\n",
    "        weights = [0.3, 0.3, 0.4]\n",
    "        for i in range(len(NPV)):\n",
    "            dic = {}\n",
    "            dic[\"Address\"] = list(NPV.keys())[i]\n",
    "            dic[\"Normalized PNL To Volume\"] = list(NPV.values())[i]\n",
    "            dic[\"Normalized PNL To Collateral\"] = list(NPC.values())[i]\n",
    "            dic[\"Normalized Expected Return\"] = list(NER.values())[i]\n",
    "            dic[\"Weighted Value\"] = (\n",
    "                list(NPV.values())[i] * weights[0] +\n",
    "                list(NPC.values())[i] * weights[1] +\n",
    "                list(NER.values())[i] * weights[2]\n",
    "            )\n",
    "            data.append(dic)\n",
    "        with open(file, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "    \n",
    "    # Function to run multiple simulations with different weights and find the best combination\n",
    "    def runMultiple(self, simulations=100):\n",
    "        NPV = self.Normalized_Pnltovolume_y()\n",
    "        NPC = self.Normalized_Pnltocollateral()\n",
    "        NER = self.Normalized_Expected_return_y()\n",
    "        data = []\n",
    "        for j in range(simulations):\n",
    "            weights = generate_weights(3, 0.275, 0.425)\n",
    "            dic = {}\n",
    "            for key in NPV.keys():\n",
    "                dic[key] = (\n",
    "                    NPV[key] * weights[0] +\n",
    "                    NPC[key] * weights[1] +\n",
    "                    NER[key] * weights[2]\n",
    "                )\n",
    "            data.append(dic)\n",
    "        sharpeRatios = self.Normalized_Sharpe_Ratio()\n",
    "        sharpeRatios = {key: sharpeRatios[key] for key in data[0]}\n",
    "        curr, currentMinimumMSE = None, float('inf')\n",
    "        for nvs in data:\n",
    "            totalSquaredError = sum([(nvs[key] - sharpeRatios[key]) ** 2 for key, value in nvs.items()])\n",
    "            meanSquaredError = totalSquaredError / len(nvs) \n",
    "            if meanSquaredError < currentMinimumMSE:\n",
    "                currentMinimumMSE, curr = meanSquaredError, nvs\n",
    "        print(weights)\n",
    "        \n",
    "    # Function to rank the weights based on their mean squared errors\n",
    "    def rankWeights(self):\n",
    "        NPV = self.Normalized_Pnltovolume_y()\n",
    "        NPC = self.Normalized_Pnltocollateral()\n",
    "        NER = self.Normalized_Expected_return_y()\n",
    "        sharpeRatios = self.Normalized_Sharpe_Ratio()\n",
    "        sharpeRatios = {key: sharpeRatios[key] for key in NPV.keys()}\n",
    "        \n",
    "        dictionary = {}\n",
    "        dictionary[\"NPV\"] = sum([(NPV[key] - sharpeRatios[key]) ** 2 for key in NPV.keys()]) / len(NPV)\n",
    "        dictionary[\"NPC\"] = sum([(NPC[key] - sharpeRatios[key]) ** 2 for key in NPC.keys()]) / len(NPC)\n",
    "        dictionary[\"NER\"] = sum([(NER[key] - sharpeRatios[key]) ** 2 for key in NER.keys()]) / len(NER)\n",
    "        \n",
    "        # Sort the weights based on mean squared errors\n",
    "        dictionary = dict(sorted(dictionary.items(), key=lambda item: item[1]))\n",
    "        \n",
    "        for i, item in enumerate(dictionary.items()):\n",
    "            print(f\"{i + 1} - {item[0]}:{item[1]}\")\n",
    "            \n",
    "            \n",
    "# Create an instance of the ProfitabilityTest class with the provided DataFrame 'df'\n",
    "PT = ProfitabilityTest(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
