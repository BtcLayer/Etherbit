{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c1751e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import statistics\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from scipy.stats import kurtosis, skew, linregress\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88eb9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/98 final.csv')\n",
    "# df = pd.read_csv('./datasets/98 gains final.csv')\n",
    "# df = pd.read_csv('./datasets/98 mux.csv')\n",
    "df = df.loc[:, df.columns.intersection([\"block_time\", \"address\", \"volume\", \"fee\", \"pnl\", \"return\", \"liquidation\"])]\n",
    "# openDF = 'unfiltered_open_and_close.csv'\n",
    "# collateralDF = 'gmx_collateral_data.csv'\n",
    "df[\"block_time\"] = df[\"block_time\"].apply(lambda x: x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49981374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(values):\n",
    "    min_value, max_value = min(values), max(values)\n",
    "    normalized_values = [(value - min_value) / (max_value - min_value) for value in values]\n",
    "    return normalized_values\n",
    "\n",
    "def generate_weights(n, min_value=0.0, max_value=1.0):\n",
    "    if min_value * n > 1 or max_value * n < 1:\n",
    "        return \"Weights are not feasible.\"\n",
    "\n",
    "    remaining_weights = 1\n",
    "    weights = []\n",
    "\n",
    "    for _ in range(n - 1):\n",
    "        weight = random.uniform(min_value, min(remaining_weights, max_value))\n",
    "        weights.append(weight)\n",
    "        remaining_weights -= weight\n",
    "\n",
    "    weights.append(remaining_weights)  # Add the last weight to ensure they sum to 1\n",
    "    random.shuffle(weights)  # Shuffle the weights to randomize their order\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e339cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionModel(tradesData, tradersData):\n",
    "    \n",
    "    cumulativeReturnTraders = {};\n",
    "    \n",
    "    for i in range(len(dfGrade)):\n",
    "        address = dfGrade.iloc[i]['Unnamed: 0'];\n",
    "        tradeReturn = dfTrade.loc[dfTrade['address'] == address]['return'].sum();\n",
    "        cumulativeReturnTraders[tradeReturn] = [dfGrade.iloc[i]['Distribution Test'], dfGrade.iloc[i]['Normal Test'], dfGrade.iloc[i]['Risk Test'], dfGrade.iloc[i]['Profitability Test']];\n",
    "    \n",
    "    y = [key for key in cumulativeReturnTraders.keys()];\n",
    "    \n",
    "    XDist = np.array([item[0] for key, item in cumulativeReturnTraders.items()]).reshape(-1, 1);\n",
    "    XDist_train, XDist_test, y_train, y_test = train_test_split(XDist, y, test_size=0.3, random_state=1)\n",
    "    regDist = linear_model.LinearRegression()\n",
    "    regDist.fit(XDist_train, y_train)\n",
    "    scoreDist = mean_absolute_error(regDist.predict(XDist_test), y_test)\n",
    "    \n",
    "    XNorm = np.array([item[1] for key, item in cumulativeReturnTraders.items()]).reshape(-1, 1);\n",
    "    XNorm_train, XNorm_test, y_train, y_test = train_test_split(XNorm, y, test_size=0.3, random_state=1)\n",
    "    regNorm = linear_model.LinearRegression()\n",
    "    regNorm.fit(XNorm_train, y_train)\n",
    "    scoreNorm = mean_absolute_error(regNorm.predict(XNorm_test), y_test)\n",
    "    \n",
    "    XRisk = np.array([item[2] for key, item in cumulativeReturnTraders.items()]).reshape(-1, 1);\n",
    "    XRisk_train, XRisk_test, y_train, y_test = train_test_split(XRisk, y, test_size=0.3, random_state=1)\n",
    "    regRisk = linear_model.LinearRegression()\n",
    "    regRisk.fit(XRisk_train, y_train)\n",
    "    scoreRisk = mean_absolute_error(regRisk.predict(XRisk_test), y_test)\n",
    "    \n",
    "    XProf = np.array([item[3] for key, item in cumulativeReturnTraders.items()]).reshape(-1, 1);\n",
    "    XProf_train, XProf_test, y_train, y_test = train_test_split(XProf, y, test_size=0.3, random_state=1)\n",
    "    regProf = linear_model.LinearRegression()\n",
    "    regProf.fit(XProf_train, y_train)\n",
    "    scoreProf = mean_absolute_error(regProf.predict(XProf_test), y_test)\n",
    "    \n",
    "    if min(scoreDist, scoreNorm, scoreRisk, scoreProf) == scoreDist:\n",
    "        return \"Distribution Test is the best for the given data\";\n",
    "    elif min(scoreDist, scoreNorm, scoreRisk, scoreProf) == scoreNorm:\n",
    "        return \"Normal Test is the best for the given data\";\n",
    "    elif min(scoreDist, scoreNorm, scoreRisk, scoreProf) == scoreRisk:\n",
    "        return \"Risk Test is the best for the given data\";\n",
    "    else:\n",
    "        return \"Profitability Test is the best for the given data\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c9a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTest:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def calculate_regression_coefficient(self, address):\n",
    "        df_filtered = self.data[self.data[\"address\"] == address]\n",
    "        x = np.arange(1, len(df_filtered) + 1)\n",
    "        y = np.cumsum(df_filtered['return'])\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        normalized_slope = (np.rad2deg(np.arctan(slope)) + 90) / 180\n",
    "        coefficient = (r_value ** 2 * normalized_slope) / (std_err + 1)\n",
    "        coefficient /= (1 - p_value) if 1 - p_value != 0 else 1\n",
    "        return coefficient\n",
    "\n",
    "    def Normalized_Regression_coefficient(self):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        regression_coefficient_values = [self.calculate_regression_coefficient(address) for address in addresses]\n",
    "        normalized_regression_coefficient_values = normalize_values(regression_coefficient_values)\n",
    "        return dict(zip(addresses, normalized_regression_coefficient_values))\n",
    "    \n",
    "    def addToCSV(self, file):\n",
    "        data = []\n",
    "        NRC = self.Normalized_Regression_coefficient()\n",
    "        for i in range(len(NRC)):\n",
    "            dic = {}\n",
    "            dic[\"Address\"] = list(NRC.keys())[i]\n",
    "            dic[\"Normalized Regression Coefficient\"] = list(NRC.values())[i]\n",
    "            data.append(dic)\n",
    "        with open(file, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "            \n",
    "RT = RegressionTest(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0df701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - NLC:0.047993521956239016\n",
      "2 - NCVNR:0.4523533051742929\n",
      "3 - NCV:0.7907727419680378\n",
      "4 - NV:0.8839590500515858\n"
     ]
    }
   ],
   "source": [
    "class RiskTest:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def calculate_negative_CV_returns(self, address):\n",
    "        returns = self.data[self.data[\"address\"] == address][\"return\"].values.tolist()\n",
    "        negative_returns = [r for r in returns if r < 0]\n",
    "        mean_negative_returns = np.mean(negative_returns)\n",
    "        std_dev_negative_returns = np.std(negative_returns)\n",
    "        cv_negative_returns = std_dev_negative_returns / mean_negative_returns\n",
    "        return cv_negative_returns\n",
    "        \n",
    "    def Normalized_CV_negative_returns(self):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        cv_negative_returns = [self.calculate_negative_CV_returns(address) for address in addresses]\n",
    "        normalized_cv_negative_returns = normalize_values(cv_negative_returns)\n",
    "        return dict(zip(addresses, normalized_cv_negative_returns))\n",
    "\n",
    "    def calculate_var(self, address, confidence_level):\n",
    "        df_filtered = self.data[self.data[\"address\"] == address]\n",
    "        returns = np.array(df_filtered[\"return\"])\n",
    "        sorted_returns = np.sort(returns)\n",
    "        index = int(np.floor(confidence_level * len(sorted_returns)))\n",
    "        var = sorted_returns[index]\n",
    "        return var\n",
    "\n",
    "    def Normalized_VaR(self, confidence_level=0.05):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        var_values = [self.calculate_var(address, confidence_level) for address in addresses]\n",
    "        normalized_var_values = normalize_values(var_values)\n",
    "        return dict(zip(addresses, normalized_var_values))\n",
    "    \n",
    "    def calculate_cvar(self, address, confidence_level):\n",
    "        df_filtered = self.data[(self.data[\"address\"] == address) & (self.data[\"pnl\"] < 0)]\n",
    "        returns = np.array(df_filtered[\"return\"])\n",
    "        cvar = np.nan if len(returns) == 0 else np.mean(returns)\n",
    "        return cvar\n",
    "\n",
    "    def Normalized_cVaR(self, confidence_level=0.05):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        cvar_values = [self.calculate_cvar(address, confidence_level) for address in addresses]\n",
    "        normalized_cvar_values = normalize_values(cvar_values)\n",
    "        return dict(zip(addresses, normalized_cvar_values))\n",
    "    \n",
    "    def calculate_liquidation_chance(self, address):\n",
    "        df_filtered = self.data[self.data[\"address\"] == address]\n",
    "        total_trades = len(df_filtered)\n",
    "        liquidation_counts = df_filtered[\"liquidation\"].apply(lambda x: 1 if x == \"Yes\" else 0).sum()\n",
    "        liquidation_chance = liquidation_counts / total_trades\n",
    "        return liquidation_chance\n",
    "\n",
    "    def Normalized_Liquidation_chance(self):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        liquidation_chance_values = [self.calculate_liquidation_chance(address) for address in addresses]\n",
    "        normalized_liquidation_chance_values = normalize_values(liquidation_chance_values)    \n",
    "        return dict(zip(addresses, normalized_liquidation_chance_values))\n",
    "    \n",
    "    def calculate_max_drawdown(self, address):\n",
    "        filtered_data = self.data[self.data['address'] == address]\n",
    "        cumulative_returns = np.cumprod(1 + filtered_data['return']) - 1\n",
    "        peaks = np.maximum.accumulate(cumulative_returns)\n",
    "        troughs = np.minimum.accumulate(cumulative_returns)\n",
    "        drawdowns = peaks - troughs\n",
    "        max_drawdown = np.max(drawdowns)\n",
    "        return max_drawdown\n",
    "    \n",
    "    def Normalized_Max_Drawdown(self):\n",
    "        values = normalize_values([self.calculate_max_drawdown(address) for address in self.data[\"address\"].unique()])\n",
    "        risk_df = {}\n",
    "        for i, address in enumerate(self.data[\"address\"].unique()):\n",
    "            risk_df[address] = values[i]\n",
    "        return risk_df\n",
    "    \n",
    "    def addToCSV(self, file):\n",
    "        data = []\n",
    "        NCVNR = self.Normalized_CV_negative_returns()\n",
    "        NV = self.Normalized_VaR()\n",
    "        NCV = self.Normalized_cVaR()\n",
    "        NLC = self.Normalized_Liquidation_chance()\n",
    "        current = 0\n",
    "        weights = [0.3, 0.3, 0.3, 0.1]\n",
    "        dic = {}\n",
    "        for i in range(len(NV)):\n",
    "            dic[\"Address\"] = list(NV.keys())[i]\n",
    "            dic[\"Normalized VaR\"] = list(NV.values())[i]\n",
    "            dic[\"Normalized CVaR\"] = list(NCV.values())[i]\n",
    "            dic[\"Normalized Liquidation Chance\"] = list(NLC.values())[i]\n",
    "            dic[\"Weighted\"] = list(NV.values())[i] * weights[1] + list(NCV.values())[i] * weights[2] + list(NLC.values())[i] * weights[0]\n",
    "        with open(file, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=dic[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(dic)\n",
    "\n",
    "    def runMultiple(self, simulations=100):\n",
    "        NCVNR = self.Normalized_CV_negative_returns()\n",
    "        NV = self.Normalized_VaR()\n",
    "        NCV = self.Normalized_cVaR()\n",
    "        NLC = self.Normalized_Liquidation_chance()\n",
    "        data = []\n",
    "        for j in range(simulations):\n",
    "            weights = generate_weights(4, 0.20, 0.45)\n",
    "            dic = {}\n",
    "            for key in NCV.keys():\n",
    "                dic[key] = NCVNR[key] * weights[0] + NV[key] * weights[1] + NCV[key] * weights[2] + NLC[key] * weights[3]\n",
    "            data.append(dic)\n",
    "        drawdowns = self.Normalized_Max_Drawdown()\n",
    "        drawdowns = {key: drawdowns[key] for key in data[0]}\n",
    "        curr, currentMinimumMSE = None, float('inf')\n",
    "        for nvs in data:\n",
    "            totalSquaredError = sum([(nvs[key] - drawdowns[key]) ** 2 for key, value in nvs.items()])\n",
    "            meanSquaredError = totalSquaredError / len(nvs) \n",
    "            if meanSquaredError < currentMinimumMSE: currentMinimumMSE, curr = meanSquaredError, nvs\n",
    "        print(weights)\n",
    "        \n",
    "    def rankWeights(self):\n",
    "        NCVNR = self.Normalized_CV_negative_returns()\n",
    "        NV = self.Normalized_VaR()\n",
    "        NCV = self.Normalized_cVaR()\n",
    "        NLC = self.Normalized_Liquidation_chance()\n",
    "        drawdowns = self.Normalized_Max_Drawdown()\n",
    "        drawdowns = {key: drawdowns[key] for key in NV.keys()}\n",
    "        \n",
    "        dictionary = {}\n",
    "        dictionary[\"NCVNR\"] = sum([(NCVNR[key] - drawdowns[key]) ** 2 for key in NCVNR.keys()]) / len(NCVNR)\n",
    "        dictionary[\"NV\"] = sum([(NV[key] - drawdowns[key]) ** 2 for key in NV.keys()]) / len(NV)\n",
    "        dictionary[\"NCV\"] = sum([(NCV[key] - drawdowns[key]) ** 2 for key in NCV.keys()]) / len(NCV)\n",
    "        dictionary[\"NLC\"] = sum([(NLC[key] - drawdowns[key]) ** 2 for key in NLC.keys()]) / len(NLC)\n",
    "        dictionary = dict(sorted(dictionary.items(), key=lambda item: item[1]))\n",
    "        \n",
    "        for i, item in enumerate(dictionary.items()):\n",
    "            print(f\"{i + 1} - {item[0]}:{item[1]}\")\n",
    "        \n",
    "\n",
    "RT = RiskTest(df)\n",
    "RT.rankWeights()\n",
    "# RT.addToCSV(\"Risk Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63f76931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - NPV:0.021273964493066882\n",
      "2 - NPC:0.021273964493066892\n",
      "3 - NER:0.028792707927005372\n"
     ]
    }
   ],
   "source": [
    "class ProfitabilityTest:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def calculate_pnltovolume(self, address):\n",
    "        df_filtered = self.data[self.data[\"address\"] == address]\n",
    "        if len(df_filtered) == 0: return None\n",
    "        total_pnl = sum(df_filtered['pnl'])\n",
    "        total_volume = sum(df_filtered['volume'])    \n",
    "        return total_pnl / total_volume\n",
    "\n",
    "    def Normalized_Pnltovolume_y(self):\n",
    "        addresses = self.data['address'].unique()\n",
    "        pnltovolume_values = [self.calculate_pnltovolume(address) for address in addresses]\n",
    "        normalized_pnltovolume_values = normalize_values(pnltovolume_values)\n",
    "        return dict(zip(addresses, normalized_pnltovolume_values))\n",
    "    \n",
    "    def calculate_pnltocollateral(self, address):\n",
    "        grouped = self.data.groupby(\"address\").agg({\"pnl\": \"sum\", \"volume\": \"sum\"})\n",
    "        grouped[\"pnltocollateral\"] = grouped[\"pnl\"] / grouped[\"volume\"]\n",
    "        return grouped.loc[address, \"pnltocollateral\"]\n",
    "\n",
    "    def Normalized_Pnltocollateral(self):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        pnltocollateral_values = [self.calculate_pnltocollateral(address) for address in addresses]\n",
    "        normalized_pnltocollateral_values = normalize_values(pnltocollateral_values)\n",
    "        return dict(zip(addresses, normalized_pnltocollateral_values))\n",
    "    \n",
    "    def calculate_expected_return(self, address):\n",
    "        expected_return = self.data[self.data[\"address\"] == address][\"return\"].mean()\n",
    "        return expected_return\n",
    "\n",
    "    def Normalized_Expected_return_y(self):\n",
    "        addresses = self.data[\"address\"].unique()\n",
    "        expected_return_values = [self.calculate_expected_return(address) for address in addresses]\n",
    "        normalized_expected_return_values = normalize_values(expected_return_values)\n",
    "        return dict(zip(addresses, normalized_expected_return_values))\n",
    "    \n",
    "    def calculate_sharpe_ratio(self, address, risk_free_rate=0.02):\n",
    "        filtered_data = self.data[self.data['address'] == address]\n",
    "        excess_returns = filtered_data['return'] - risk_free_rate\n",
    "        sharpe_ratio = np.mean(excess_returns) / np.std(excess_returns)\n",
    "        return sharpe_ratio\n",
    "    \n",
    "    def Normalized_Sharpe_Ratio(self):\n",
    "        values = normalize_values([self.calculate_sharpe_ratio(address) for address in self.data[\"address\"].unique()])\n",
    "        profitability_df = {}\n",
    "        for i, address in enumerate(self.data[\"address\"].unique()):\n",
    "            profitability_df[address] = values[i]\n",
    "        return profitability_df\n",
    "    \n",
    "    def addToCSV(self, file):\n",
    "        data = []\n",
    "        NPV = self.Normalized_Pnltovolume_y()\n",
    "        NPC = self.Normalized_Pnltocollateral()\n",
    "        NER = self.Normalized_Expected_return_y()\n",
    "        current = 0\n",
    "        weights = [0.3, 0.3, 0.4]\n",
    "        for i in range(len(NPV)):\n",
    "            dic = {}\n",
    "            dic[\"Address\"] = list(NPV.keys())[i]\n",
    "            dic[\"Normalized PNL To Volume\"] = list(NPV.values())[i]\n",
    "            dic[\"Normalized PNL To Collateral\"] = list(NPC.values())[i]\n",
    "            dic[\"Normalized Expected Return\"] = list(NER.values())[i]\n",
    "            dic[\"Weighted Value\"] = list(NPV.values())[i] * weights[0] + list(NPC.values())[i] * weights[1] + list(NER.values())[i] * weights[2]\n",
    "            data.append(dic)\n",
    "        with open(file, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "    \n",
    "    def runMultiple(self, simulations=100):\n",
    "        NPV = self.Normalized_Pnltovolume_y()\n",
    "        NPC = self.Normalized_Pnltocollateral()\n",
    "        NER = self.Normalized_Expected_return_y()\n",
    "        data = []\n",
    "        for j in range(simulations):\n",
    "            weights = generate_weights(3, 0.275, 0.425)\n",
    "            dic = {}\n",
    "            for key in NPV.keys():\n",
    "                dic[key] = NPV[key] * weights[0] + NPC[key] * weights[1] + NER[key] * weights[2]\n",
    "            data.append(dic)\n",
    "        sharpeRatios = self.Normalized_Sharpe_Ratio()\n",
    "        sharpeRatios = {key: sharpeRatios[key] for key in data[0]}\n",
    "        curr, currentMinimumMSE = None, float('inf')\n",
    "        for nvs in data:\n",
    "            totalSquaredError = sum([(nvs[key] - sharpeRatios[key]) ** 2 for key, value in nvs.items()])\n",
    "            meanSquaredError = totalSquaredError / len(nvs) \n",
    "            if meanSquaredError < currentMinimumMSE: currentMinimumMSE, curr = meanSquaredError, nvs\n",
    "        print(weights)\n",
    "        \n",
    "    def rankWeights(self):\n",
    "        NPV = self.Normalized_Pnltovolume_y()\n",
    "        NPC = self.Normalized_Pnltocollateral()\n",
    "        NER = self.Normalized_Expected_return_y()\n",
    "        sharpeRatios = self.Normalized_Sharpe_Ratio()\n",
    "        sharpeRatios = {key: sharpeRatios[key] for key in NPV.keys()}\n",
    "        \n",
    "        dictionary = {}\n",
    "        dictionary[\"NPV\"] = sum([(NPV[key] - sharpeRatios[key]) ** 2 for key in NPV.keys()]) / len(NPV)\n",
    "        dictionary[\"NPC\"] = sum([(NPC[key] - sharpeRatios[key]) ** 2 for key in NPC.keys()]) / len(NPC)\n",
    "        dictionary[\"NER\"] = sum([(NER[key] - sharpeRatios[key]) ** 2 for key in NER.keys()]) / len(NER)\n",
    "        dictionary = dict(sorted(dictionary.items(), key=lambda item: item[1]))\n",
    "        \n",
    "        for i, item in enumerate(dictionary.items()):\n",
    "            print(f\"{i + 1} - {item[0]}:{item[1]}\")\n",
    "            \n",
    "            \n",
    "PT = ProfitabilityTest(df)\n",
    "PT.rankWeights()\n",
    "PT.addToCSV(\"Profitability Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bfa22d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'addresses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m max_drawdown\n\u001b[0;32m     10\u001b[0m risk_df \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Drawdown\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]}\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m address \u001b[38;5;129;01min\u001b[39;00m \u001b[43maddresses\u001b[49m:\n\u001b[0;32m     12\u001b[0m     risk_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(address)\n\u001b[0;32m     13\u001b[0m     risk_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Drawdown\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(calculate_max_drawdown(df, address))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'addresses' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_max_drawdown(data, address):\n",
    "    filtered_data = data[data['address'] == address]\n",
    "    cumulative_returns = np.cumprod(1 + filtered_data['return']) - 1\n",
    "    peaks = np.maximum.accumulate(cumulative_returns)\n",
    "    troughs = np.minimum.accumulate(cumulative_returns)\n",
    "    drawdowns = peaks - troughs\n",
    "    max_drawdown = np.max(drawdowns)\n",
    "    return max_drawdown\n",
    "\n",
    "risk_df = {\"address\":[], \"Max Drawdown\":[]}\n",
    "for address in addresses:\n",
    "    risk_df[\"address\"].append(address)\n",
    "    risk_df[\"Max Drawdown\"].append(calculate_max_drawdown(df, address))\n",
    "risk_df[\"Max Drawdown\"] = normalize_values(risk_df[\"Max Drawdown\"])\n",
    "# risk_df = pd.DataFrame(risk_df)\n",
    "# risk_df.to_csv('Risk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25694fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['block_time'] = pd.to_datetime(df['block_time'])\n",
    "average_monthly_returns = df.groupby(['address', df['block_time'].dt.to_period('M')])['return'].mean()\n",
    "# std_dev_returns = df.groupby(['address', df['block_time'].dt.to_period('M')])['return'].std()\n",
    "# avg_returns_factor_std = average_monthly_returns / std_dev_returns\n",
    "# avg_returns_factor_std\n",
    "dfTemp = pd.DataFrame({'address': average_monthly_returns.index.get_level_values('address'),\n",
    "                              'Month': average_monthly_returns.index.get_level_values('block_time'),\n",
    "                              'Average Income': average_monthly_returns})\n",
    "# dfTemp = pd.read_csv('Regression.csv')\n",
    "addresses = dfTemp[\"address\"].unique()\n",
    "regression_df = {\"address\" : addresses, \"Average Monthly\" : []}\n",
    "regression_df[\"Average Monthly\"] = normalize_values([dfTemp[dfTemp[\"address\"] == address][\"Average Income\"].mean() for address in addresses])\n",
    "# regression_df = pd.DataFrame(regression_df)\n",
    "# regression_df.to_csv('Regression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb755530",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data.update(profitability_df)\n",
    "data.update(risk_df)\n",
    "data.update(regression_df)\n",
    "data = pd.DataFrame(data)\n",
    "data.to_csv('Combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Combined.csv')\n",
    "df['z_max_drawdown'] = (df['Max Drawdown'] - df['Max Drawdown'].mean()) / df['Max Drawdown'].std()\n",
    "df['z_sharpe_ratio'] = (df['Sharpe Ratio'] - df['Sharpe Ratio'].mean()) / df['Sharpe Ratio'].std()\n",
    "df['z_avg_monthly_returns'] = (df['Average Monthly'] - df['Average Monthly'].mean()) / df['Average Monthly'].std()\n",
    "\n",
    "weights = generate_weights(3, 0.3, 0.4)\n",
    "\n",
    "# Calculate weighted sum of standardized metrics\n",
    "df['weighted_sum'] = (weights[0] * df['z_max_drawdown']) + (weights[1] * df['z_sharpe_ratio']) + (weights[2] * df['z_avg_monthly_returns'])\n",
    "\n",
    "# df['rank'] = df['weighted_sum'].rank(ascending=False)\n",
    "# sorted_addresses = df.sort_values('rank')['address'].tolist()\n",
    "# sorted_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f060747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfPT = pd.read_csv(\"Profitability Test.csv\")\n",
    "# PTDict = {address:dfPT[dfPT[\"Address\"] == address][\"Weighted Value\"] for address in dfPT[\"Address\"].unique()}\n",
    "# SRDict = {address:df[df[\"address\"] == address][\"weighted_sum\"] for address in df[\"address\"].unique()}\n",
    "# SRDict = {key: SRDict[key] for key in PTDict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880fee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5465eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Given target values as a dictionary\n",
    "# target_values = {\"A\": 10, \"B\": 20, \"C\": 30, \"D\": 40, \"E\": 50}\n",
    "\n",
    "# # Simulated calculated values from multiple iterations\n",
    "# calculated_values = [\n",
    "#     {\"A\": 12, \"B\": 18, \"C\": 28, \"D\": 43, \"E\": 55},\n",
    "#     {\"A\": 11, \"B\": 21, \"C\": 29, \"D\": 39, \"E\": 49},\n",
    "#     {\"A\": 9, \"B\": 19, \"C\": 32, \"D\": 42, \"E\": 52},\n",
    "#     {\"A\": 10, \"B\": 20, \"C\": 30, \"D\": 40, \"E\": 50},  # Closest to the target values\n",
    "#     {\"A\": 14, \"B\": 22, \"C\": 31, \"D\": 37, \"E\": 45}\n",
    "# ]\n",
    "\n",
    "# # Calculate the sum of absolute differences for each calculated value\n",
    "# curr, currentMinimumMSE = None, float('inf')\n",
    "# for calculated in calculated_values:\n",
    "#     totalSquaredError = sum([(calculated[key] - target_values[key]) ** 2 for key, value in calculated.items()])\n",
    "#     meanSquaredError = totalSquaredError / len(calculated)  # Calculate the mean\n",
    "#     if meanSquaredError < currentMinimumMSE: currentMinimumMSE, curr = meanSquaredError, calculated\n",
    "\n",
    "# print(\"Closest calculated value:\", curr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(generate_weights(3, 0.19, 0.36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd54dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
